<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>从 Linear Model 到 Stacking，我是如何将 RMSLE 优化至 0.12 的？ | Adam8enの8log</title><meta name="author" content="Adam Ben"><meta name="copyright" content="Adam Ben"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="Kaggle 房价预测实战复盘。记录从 Linear Model (0.16) 到 Stacking (0.12) 的优化全流程。">
<meta property="og:type" content="article">
<meta property="og:title" content="从 Linear Model 到 Stacking，我是如何将 RMSLE 优化至 0.12 的？">
<meta property="og:url" content="https://adamben.top/posts/2ed6902d017c/index.html">
<meta property="og:site_name" content="Adam8enの8log">
<meta property="og:description" content="Kaggle 房价预测实战复盘。记录从 Linear Model (0.16) 到 Stacking (0.12) 的优化全流程。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20260115192208544.png?x-oss-process=style/blog">
<meta property="article:published_time" content="2026-01-15T12:06:12.000Z">
<meta property="article:modified_time" content="2026-01-15T12:06:12.000Z">
<meta property="article:author" content="Adam Ben">
<meta property="article:tag" content="Linear Model">
<meta property="article:tag" content="MLP">
<meta property="article:tag" content="XGBoost">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20260115192208544.png?x-oss-process=style/blog"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="https://adamben.top/posts/2ed6902d017c/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '从 Linear Model 到 Stacking，我是如何将 RMSLE 优化至 0.12 的？',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-01-15 20:06:12'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="https://npm.elemecdn.com/lxgw-wenkai-screen-webfont/style.css" media="print" onload="this.media='all'"><svg aria-hidden="true" style="position:absolute; overflow:hidden; width:0; height:0"><symbol id="icon-sun" viewBox="0 0 1024 1024"><path d="M960 512l-128 128v192h-192l-128 128-128-128H192v-192l-128-128 128-128V192h192l128-128 128 128h192v192z" fill="#FFD878" p-id="8420"></path><path d="M736 512a224 224 0 1 0-448 0 224 224 0 1 0 448 0z" fill="#FFE4A9" p-id="8421"></path><path d="M512 109.248L626.752 224H800v173.248L914.752 512 800 626.752V800h-173.248L512 914.752 397.248 800H224v-173.248L109.248 512 224 397.248V224h173.248L512 109.248M512 64l-128 128H192v192l-128 128 128 128v192h192l128 128 128-128h192v-192l128-128-128-128V192h-192l-128-128z" fill="#4D5152" p-id="8422"></path><path d="M512 320c105.888 0 192 86.112 192 192s-86.112 192-192 192-192-86.112-192-192 86.112-192 192-192m0-32a224 224 0 1 0 0 448 224 224 0 0 0 0-448z" fill="#4D5152" p-id="8423"></path></symbol><symbol id="icon-moon" viewBox="0 0 1024 1024"><path d="M611.370667 167.082667a445.013333 445.013333 0 0 1-38.4 161.834666 477.824 477.824 0 0 1-244.736 244.394667 445.141333 445.141333 0 0 1-161.109334 38.058667 85.077333 85.077333 0 0 0-65.066666 135.722666A462.08 462.08 0 1 0 747.093333 102.058667a85.077333 85.077333 0 0 0-135.722666 65.024z" fill="#FFB531" p-id="11345"></path><path d="M329.728 274.133333l35.157333-35.157333a21.333333 21.333333 0 1 0-30.165333-30.165333l-35.157333 35.157333-35.114667-35.157333a21.333333 21.333333 0 0 0-30.165333 30.165333l35.114666 35.157333-35.114666 35.157334a21.333333 21.333333 0 1 0 30.165333 30.165333l35.114667-35.157333 35.157333 35.157333a21.333333 21.333333 0 1 0 30.165333-30.165333z" fill="#030835" p-id="11346"></path></symbol></svg><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@1.0.17/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Adam8enの8log" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">71</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">92</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw far fa-comment"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fa fa-book"></i><span> 书籍</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fas fa-gamepad"></i><span> 游戏</span></a></li><li><a class="site-page child" href="/bangumis/"><i class="fa-fw fas fa-tv"></i><span> 追番</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20260115192208544.png?x-oss-process=style/blog')"><nav id="nav"><span id="blog-info"><a href="/" title="Adam8enの8log"><span class="site-name">Adam8enの8log</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw far fa-comment"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fa fa-book"></i><span> 书籍</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fas fa-gamepad"></i><span> 游戏</span></a></li><li><a class="site-page child" href="/bangumis/"><i class="fa-fw fas fa-tv"></i><span> 追番</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">从 Linear Model 到 Stacking，我是如何将 RMSLE 优化至 0.12 的？</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2026-01-15T12:06:12.000Z" title="发表于 2026-01-15 20:06:12">2026-01-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-01-15T12:06:12.000Z" title="更新于 2026-01-15 20:06:12">2026-01-15</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Machine-Learning/">Machine-Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>36分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="从 Linear Model 到 Stacking，我是如何将 RMSLE 优化至 0.12 的？"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><div class="top-img gist" style="background-image: url(https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20260115192208544.png?x-oss-process=style/blog)"></div><article class="post-content" id="article-container"><span class='p center logo large'>Kaggle 实战复盘</span>
<span class='p center small'>从 Linear Model 到 Stacking，我是如何将 RMSLE 优化至 0.12 的？</span>
<p>笔者近期正在学习<a target="_blank" rel="noopener" href="https://zh.d2l.ai/">《动手学深度学习》</a>（后简称《D2L》）这本书，在4.10节处，书中手把手带我们用最简单的线性模型复现了Kaggle上一个预测房价的模型。我在按照书上的代码复现后，又根据Gemini的建议采取了进一步的优化建议以取得更好的效果。这篇博客旨在记录笔者的复现流程，以及一点学习心得。</p>
<p>题目的链接如下：</p>
<a class="btn-beautify block center larger" target="_blank" rel="noopener" href="https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview" 
  title="House Prices - Advanced Regression Techniques"><i class="far fa-hand-point-right"></i><span>House Prices - Advanced Regression Techniques</span></a>
<p>我们先从《D2L》这本书的做法写起吧。</p>
<h2 id="d2l的做法"><a class="markdownIt-Anchor" href="#d2l的做法"></a> 《D2L》的做法</h2>
<p>我们简单分为数据预处理与模型训练两部分来说。</p>
<h3 id="数据预处理"><a class="markdownIt-Anchor" href="#数据预处理"></a> 数据预处理</h3>
<p>首先，我们先把Kaggle房屋数据集下载到本地，并用pandas库加载训练数据集与测试数据集。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line">DATA_HUB = <span class="built_in">dict</span>()</span><br><span class="line">DATA_URL = <span class="string">&#x27;http://d2l-data.s3-accelerate.amazonaws.com/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download</span>(<span class="params">name, cache_dir=os.path.join(<span class="params"><span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span></span>)</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载一个DATA_HUB中的文件，返回本地文件名&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> name <span class="keyword">in</span> DATA_HUB, <span class="string">f&quot;<span class="subst">&#123;name&#125;</span> 不存在于 <span class="subst">&#123;DATA_HUB&#125;</span>&quot;</span></span><br><span class="line">    url, sha1_hash = DATA_HUB[name]</span><br><span class="line">    os.makedirs(cache_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    fname = os.path.join(cache_dir, url.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> os.path.exists(fname):</span><br><span class="line">        sha1 = hashlib.sha1()</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                data = f.read(<span class="number">1048576</span>)</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> data:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                sha1.update(data)</span><br><span class="line">        <span class="keyword">if</span> sha1.hexdigest() == sha1_hash:</span><br><span class="line">            <span class="keyword">return</span> fname  <span class="comment"># 命中缓存</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;正在从<span class="subst">&#123;url&#125;</span>下载<span class="subst">&#123;fname&#125;</span>...&#x27;</span>)</span><br><span class="line">    r = requests.get(url, stream=<span class="literal">True</span>, verify=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(r.content)</span><br><span class="line">    <span class="keyword">return</span> fname</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_extract</span>(<span class="params">name, folder=<span class="literal">None</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载并解压zip/tar文件&quot;&quot;&quot;</span></span><br><span class="line">    fname = download(name)</span><br><span class="line">    base_dir = os.path.dirname(fname)</span><br><span class="line">    data_dir, ext = os.path.splitext(fname)</span><br><span class="line">    <span class="keyword">if</span> ext == <span class="string">&#x27;.zip&#x27;</span>:</span><br><span class="line">        fp = zipfile.ZipFile(fname, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> ext <span class="keyword">in</span> (<span class="string">&#x27;.tar&#x27;</span>, <span class="string">&#x27;.gz&#x27;</span>):</span><br><span class="line">        fp = tarfile.<span class="built_in">open</span>(fname, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="literal">False</span>, <span class="string">&#x27;只有zip/tar文件可以被解压缩&#x27;</span></span><br><span class="line">    fp.extractall(base_dir)</span><br><span class="line">    <span class="keyword">return</span> os.path.join(base_dir, folder) <span class="keyword">if</span> folder <span class="keyword">else</span> data_dir</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_all</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载DATA_HUB中的所有文件&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> DATA_HUB:</span><br><span class="line">        download(name)</span><br><span class="line"></span><br><span class="line">DATA_HUB[<span class="string">&#x27;kaggle_house_train&#x27;</span>] = (  <span class="comment">#@save</span></span><br><span class="line">    DATA_URL + <span class="string">&#x27;kaggle_house_pred_train.csv&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;585e9cc93e70b39160e7921475f9bcd7d31219ce&#x27;</span>)</span><br><span class="line"></span><br><span class="line">DATA_HUB[<span class="string">&#x27;kaggle_house_test&#x27;</span>] = (  <span class="comment">#@save</span></span><br><span class="line">    DATA_URL + <span class="string">&#x27;kaggle_house_pred_test.csv&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;fa19780a7b011d9b009e8bff8e99922a8ee2eb90&#x27;</span>)</span><br><span class="line"></span><br><span class="line">train_data = pd.read_csv(download(<span class="string">&#x27;kaggle_house_train&#x27;</span>))</span><br><span class="line">test_data = pd.read_csv(download(<span class="string">&#x27;kaggle_house_test&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20260113154943290.png?x-oss-process=style/blog" alt="image-20260113154943290" /></p>
<p>可以看到，训练数据集包括1460条样本，每个样本有80个特征和一个标签。而测试数据集有1459条样本，每个样本包含80个特征。</p>
<p>再看看训练数据集的前四个和后两个特征，结果如下。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20260113155337806.png?x-oss-process=style/blog" alt="image-20260113155337806" /></p>
<p>可以看到，训练数据集中每个样本的第一个特征是ID值，用以唯一标识该样本。然而，在实际训练中，ID值不反映任何信息，因此我们需要将ID列从数据集中删除。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_features = pd.concat((train_data.iloc[:, <span class="number">1</span>:-<span class="number">1</span>], test_data.iloc[:, <span class="number">1</span>:])) <span class="comment"># 将第0列（ID）从数据集中删除</span></span><br></pre></td></tr></table></figure>
<h4 id="数据标准化"><a class="markdownIt-Anchor" href="#数据标准化"></a> 数据标准化</h4>
<p>我们对数据进行预处理大体上有两个任务：</p>
<ol>
<li>将所有的缺失值替换为相应特征值的平均值</li>
<li>将所有特征值重新缩放到零均值和单位方差来标准化数据</li>
</ol>
<p>这看似是两个任务，其实可以一步搞定。所谓数据标准化，其实可以参考概率论中正态分布标准化的方法，即用以下步骤得到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span>。</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>x</mi><mo>←</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><mi>σ</mi></mfrac></mrow><annotation encoding="application/x-tex">x \leftarrow \frac{x - \mu}{\sigma}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.9463300000000001em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603300000000002em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">μ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">μ</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span>分别代表样本均值与标准差。不难得出，处理后的特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span>具有零均值和单位方差，推导过程如下：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>E</mi><mo stretchy="false">[</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><mi>σ</mi></mfrac><mo stretchy="false">]</mo><mo>=</mo><mfrac><mrow><mi>μ</mi><mo>−</mo><mi>μ</mi></mrow><mi>σ</mi></mfrac><mo>=</mo><mn>0</mn><mspace linebreak="newline"></mspace><mi>E</mi><mo stretchy="false">[</mo><mo stretchy="false">(</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><mi>σ</mi></mfrac><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo stretchy="false">]</mo><mo>=</mo><mfrac><mrow><mi>E</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mo>−</mo><mn>2</mn><msup><mi>μ</mi><mn>2</mn></msup><mo>+</mo><msup><mi>μ</mi><mn>2</mn></msup></mrow><msup><mi>σ</mi><mn>2</mn></msup></mfrac><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><msup><mi>μ</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mo>−</mo><msup><mi>μ</mi><mn>2</mn></msup></mrow><msup><mi>σ</mi><mn>2</mn></msup></mfrac><mo>=</mo><mn>1</mn><mspace linebreak="newline"></mspace><mi>D</mi><mo stretchy="false">[</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><mi>σ</mi></mfrac><mo stretchy="false">]</mo><mo>=</mo><mn>1</mn><mo>−</mo><msup><mn>0</mn><mn>2</mn></msup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">E[\frac{x - \mu}{\sigma}] = \frac{\mu - \mu}{\sigma} = 0 \\
E[(\frac{x - \mu}{\sigma}) ^ 2] = \frac{E(x^2) - 2\mu^2 + \mu^2}{\sigma ^ 2} = \frac{(\sigma^2 + \mu^2) - \mu^2}{\sigma ^ 2} = 1 \\
D[\frac{x - \mu}{\sigma}] = 1 - 0^2 = 1
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.9463300000000001em;vertical-align:-0.686em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">[</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603300000000002em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">μ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.9463300000000001em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603300000000002em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">μ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.9463300000000001em;vertical-align:-0.686em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">[</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603300000000002em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">μ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.177108em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.491108em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">2</span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.177108em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.491108em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1.9463300000000001em;vertical-align:-0.686em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mopen">[</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603300000000002em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">μ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8641079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span></span></p>
<p>这么做有两个好处：首先，它方便优化。因为将所有的数值进行标准化后，特征均值就<span class="bubble-content">全部变为0</span><span class="bubble-notation"><span class="bubble-item" style="background-color:#ec5830;">均值消失</span></span>。因此，我们可以将缺失值统一设置为0，视为均值填充；其次，我们不知道哪些特征是相关的，所以我们不想让惩罚分配给一个特征的系数比分配给其他特征的系数更大。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line"><span class="comment"># 将特征进行标准化，重新缩放到零均值和单位方差</span></span><br><span class="line">numeric_features = all_features.dtypes[all_features.dtypes != <span class="string">&#x27;object&#x27;</span>].index</span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].apply(</span><br><span class="line">    <span class="keyword">lambda</span> x: (x - x.mean()) / (x.std())</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 标准化数据后，均值为0，故缺失值设置为0</span></span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].fillna(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="处理离散值"><a class="markdownIt-Anchor" href="#处理离散值"></a> 处理离散值</h4>
<p>接下来，对于离散值，如“MSZoning”之类的特征，我们可以用<span class="bubble-content">独热编码</span><span class="bubble-notation"><span class="bubble-item" style="background-color:#ec5830;">One-Hot Encoding</span></span>来替换。使用pandas库下的<code>get_dummies()</code>方法可以让我们很轻松的做到这一点。</p>
<blockquote>
<p>例如，“MSZoning”包含值“RL”和“Rm”。 我们将创建两个新的指示器特征“MSZoning_RL”和“MSZoning_RM”，其值为0或1。 根据独热编码，如果“MSZoning”的原始值为“RL”， 则：“MSZoning_RL”为1，“MSZoning_RM”为0。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 接下来，用独热编码替换离散值</span></span><br><span class="line">all_features = pd.get_dummies(all_features, dummy_na=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>顺带一提，独热编码的本质是把一个特征拆分成多个正交关系的特征。比如“街道”不能被直接拆分成“1、2、3……”，因为它们之间没有大小关系，只能用独热编码处理。</p>
<p>值得注意的是，这么做会导致数据集特征数量大幅增长，原因在于对离散值独热编码会让特征分裂。这很好理解，比如“MSZoning”会分裂为“MSZoning_RL”和“MSZoning_RM”两个新的特征。事实上，这里的数据集特征也的确从79个增加到了331个。</p>
<p>最后，通过values属性，将pandas格式的数据集提取为NumPy格式，并将其转换为张量开始训练。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从pandas格式提取NumPy格式，并转换为张量用于训练</span></span><br><span class="line">n_train = train_data.shape[<span class="number">0</span>]</span><br><span class="line">train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)</span><br><span class="line">test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)</span><br><span class="line">train_labels = torch.tensor(</span><br><span class="line">    train_data.SalePrice.values.reshape(-<span class="number">1</span>, <span class="number">1</span>),dtype=torch.float32</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="模型训练"><a class="markdownIt-Anchor" href="#模型训练"></a> 模型训练</h3>
<p>《D2L》这本书训练了一个带有损失平方的线性模型，这个模型非常基础，能力也相当一般。但它可以作为一个<span class="bubble-content">基线</span><span class="bubble-notation"><span class="bubble-item" style="background-color:#ec5830;">baseline</span></span>模型，让我们知道后续模型的能力超出了它多少。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练一个线性模型，作为baseline</span></span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line">in_features = train_features.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_net</span>():</span><br><span class="line">    net = nn.Sequential(nn.Linear(in_features,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure>
<p>一个小细节是：为了更客观的评价模型的误差，我们还需要引入<span class="bubble-content">RMSLE</span><span class="bubble-notation"><span class="bubble-item" style="background-color:#ec5830;">均方根对数误差</span></span>。</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msqrt><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">(</mo><mi>log</mi><mo>⁡</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mi>log</mi><mo>⁡</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{\frac{1}{n} \sum_{i=1}^{n} (\log y_i - \log \hat{y}_i)^2}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.1568160000000005em;vertical-align:-1.277669em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8791470000000006em;"><span class="svg-align" style="top:-5.116816em;"><span class="pstrut" style="height:5.116816em;"></span><span class="mord" style="padding-left:1.056em;"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">n</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span><span style="top:-3.8391470000000005em;"><span class="pstrut" style="height:5.116816em;"></span><span class="hide-tail" style="min-width:0.742em;height:3.196816em;"><svg width='400em' height='3.196816em' viewBox='0 0 400000 3196' preserveAspectRatio='xMinYMin slice'><path d='M702 80H40000040
H742v3062l-4 4-4 4c-.667.7 -2 1.5-4 2.5s-4.167 1.833-6.5 2.5-5.5 1-9.5 1
h-12l-28-84c-16.667-52-96.667 -294.333-240-727l-212 -643 -85 170
c-4-3.333-8.333-7.667-13 -13l-13-13l77-155 77-156c66 199.333 139 419.667
219 661 l218 661zM702 80H400000v40H742z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span></span></span></span></span></p>
<p>因为MSE关注的是绝对误差，RMSLE关注的是相对误差，显然后者才是我们需要的。打个比方，对于一栋12.5万美元的房子，我们的预测偏了10万美元，那么这个模型预测结果就很糟糕；如果是一栋400万美元的豪宅，我们的预测同样偏差了10万美元，那我们的结果就还不错。但是，对于使用MSE误差的模型来说，这两个误差程度是相同的，这显然不是我们想要的结果。所以，我们需要引入RMSLE。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">log_rmse</span>(<span class="params">net, features, labels</span>):</span><br><span class="line">    <span class="comment"># 为了在取对数时进一步稳定该值，将小于1的值设置为1</span></span><br><span class="line">    clipped_preds = torch.clamp(net(features), <span class="number">1</span>, <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br><span class="line">    rmse = torch.sqrt(loss(torch.log(clipped_preds),</span><br><span class="line">                           torch.log(labels)))</span><br><span class="line">    <span class="keyword">return</span> rmse.item()</span><br></pre></td></tr></table></figure>
<p>之后，我们引入Adam优化器和K折交叉验证来辅助训练模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_features, train_labels, test_features, test_labels,</span></span><br><span class="line"><span class="params">          num_epochs, learning_rate, weighr_decay, batch_size</span>):</span><br><span class="line">    train_ls, test_ls =[], []</span><br><span class="line">    train_iter = d2l.load_array((train_features, train_labels), batch_size)</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(),</span><br><span class="line">                                 lr=learning_rate,</span><br><span class="line">                                 weight_decay=weighr_decay)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l = loss(net(X),y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">        train_ls.append(log_rmse(net, train_features, train_labels))</span><br><span class="line">        <span class="keyword">if</span> test_labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            test_ls.append(log_rmse(net, test_features, test_labels))</span><br><span class="line">    <span class="keyword">return</span> train_ls, test_ls</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_k_fold_data</span>(<span class="params">k, i, X, y</span>):</span><br><span class="line">    <span class="keyword">assert</span> k &gt; <span class="number">1</span></span><br><span class="line">    fold_size = X.shape[<span class="number">0</span>] // k</span><br><span class="line">    X_train, y_train = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        idx = <span class="built_in">slice</span>(j * fold_size, (j+<span class="number">1</span>) * fold_size)</span><br><span class="line">        X_part, y_part = X[idx, :], y[idx]</span><br><span class="line">        <span class="keyword">if</span> j == i:</span><br><span class="line">            X_valid, y_valid = X_part, y_part</span><br><span class="line">        <span class="keyword">elif</span> X_train <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            X_train, y_train = X_part, y_part</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X_train = torch.cat([X_train, X_part], <span class="number">0</span>)</span><br><span class="line">            y_train = torch.cat([y_train, y_part], <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> X_train, y_train, X_valid, y_valid</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">k_fold</span>(<span class="params">k, X_train, y_train, num_epochs, learning_rate, weight_decay,</span></span><br><span class="line"><span class="params">           batch_size</span>):</span><br><span class="line">    train_l_sum, valid_l_sum = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        data = get_k_fold_data(k, i, X_train, y_train)</span><br><span class="line">        net = get_net()</span><br><span class="line">        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,</span><br><span class="line">                                   weight_decay, batch_size)</span><br><span class="line">        train_l_sum += train_ls[-<span class="number">1</span>]</span><br><span class="line">        valid_l_sum += valid_ls[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            d2l.plot(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>)), [train_ls, valid_ls],</span><br><span class="line">                     xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;rmse&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                     legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>], yscale=<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">            <span class="comment"># 【新增代码】将图像保存到当前目录下的 result.png 文件中</span></span><br><span class="line">            <span class="comment"># plt.savefig(&#x27;result1.png&#x27;)</span></span><br><span class="line">            <span class="comment"># print(&quot;图像已保存为 result1.png&quot;) </span></span><br><span class="line">            </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;折<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>，训练log rmse<span class="subst">&#123;<span class="built_in">float</span>(train_ls[-<span class="number">1</span>]):f&#125;</span>, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;验证log rmse<span class="subst">&#123;<span class="built_in">float</span>(valid_ls[-<span class="number">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class="line">              </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> train_l_sum / k, valid_l_sum /k</span><br></pre></td></tr></table></figure>
<p>关于Adam优化器，可以参照我之前的博客：<a href="https://adamben.top/posts/282cf3400e09/?highlight=adam">Coursera-ML-AndrewNg-Notes-Week5 | Adam8en の 8log</a>。简单来说，Adam算法就是对每个参数动态的调整它们的学习率从而一定程度上优化模型。</p>
<p>K折交叉验证是一个在本地训练和验证模型的方法。简单来说，它将训练集划分为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span>折，然后以此选择第<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathnormal">i</span></span></span></span>个切片作为验证数据，其余部分作为训练数据。</p>
<p>不过，这么做并不是处理数据的最有效方法。因为它的原理是将划分出的数据集<strong>复制了一份</strong>。当数据集很大时（比如100GB），这么做不仅耗时而且会大量占用内存。目前的工业界主流做法是用索引＋采样器，核心逻辑是不移动数据，只维护一个索引列表。如果数据大到连内存都装不下（比如1TB的文本数据），这个时候就用“流式读取”。</p>
<p>因为这道题的数据量很小（几百KB的CSV文件），所以用笨方法完全可行。</p>
<p>这里还有一个很有趣的细节：训练用的 Loss 和评估用的 Metric 是不一样的。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20260113170317785.png?x-oss-process=style/blog" alt="image-20260113170317785" /></p>
<ul>
<li><strong>教练（Optimizer）</strong>：使用的是 <strong>MSE Loss</strong>。因为它数学性质好，求导平滑，适合用来指导神经网络调整参数（反向传播）。</li>
<li><strong>裁判（Evaluation）</strong>：使用的是 <strong>RMSLE (Log RMSE)</strong>。这是 Kaggle 官方的计分标准。我们虽然优化的是 MSE，但最终必须用 RMSLE 来衡量模型在赛场上的真实表现。</li>
</ul>
<p>这就好比高考：平时的模拟题（MSE）是为了练手感，但最后录取只看高考卷面分（RMSLE）。虽然题目不一样，但能力提升了，两个分数自然都会高。</p>
<h3 id="最终结果"><a class="markdownIt-Anchor" href="#最终结果"></a> 最终结果</h3>
<p>《D2L》中提供了一组未经调优的超参数供我们训练模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">k, num_epochs, lr, weight_decay, batch_size = <span class="number">5</span>, <span class="number">100</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">64</span></span><br><span class="line">train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,</span><br><span class="line">                          weight_decay, batch_size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;k&#125;</span>-折验证: 平均训练log rmse: <span class="subst">&#123;<span class="built_in">float</span>(train_l):f&#125;</span>, &#x27;</span></span><br><span class="line">      <span class="string">f&#x27;平均验证log rmse: <span class="subst">&#123;<span class="built_in">float</span>(valid_l):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>运行后可以得到如下结果：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/result1.png?x-oss-process=style/blog" alt="result1" /></p>
<p>看样子训练的结果还不错。最后，我们在题目给定的测试数据集上用模型预测标签，输出预测结果并将其保存在一个CSV文件中，提交到Kaggle就可以查看成绩了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_and_pred</span>(<span class="params">train_features, test_features, train_labels, test_data,</span></span><br><span class="line"><span class="params">                   num_epochs, lr, weight_decay, batch_size</span>):</span><br><span class="line">    net = get_net()</span><br><span class="line">    train_ls, _ = train(net, train_features, train_labels, <span class="literal">None</span>, <span class="literal">None</span>,</span><br><span class="line">                        num_epochs, lr, weight_decay, batch_size)</span><br><span class="line">    d2l.plot(np.arange(<span class="number">1</span>, num_epochs + <span class="number">1</span>), [train_ls], xlabel=<span class="string">&#x27;epoch&#x27;</span>,</span><br><span class="line">             ylabel=<span class="string">&#x27;log rmse&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], yscale=<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&#x27;train_final.png&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;训练log rmse：<span class="subst">&#123;<span class="built_in">float</span>(train_ls[-<span class="number">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 将网络应用于测试集。</span></span><br><span class="line">    preds = net(test_features).detach().numpy()</span><br><span class="line">    <span class="comment"># 将其重新格式化以导出到Kaggle</span></span><br><span class="line">    test_data[<span class="string">&#x27;SalePrice&#x27;</span>] = pd.Series(preds.reshape(<span class="number">1</span>, -<span class="number">1</span>)[<span class="number">0</span>])</span><br><span class="line">    submission = pd.concat([test_data[<span class="string">&#x27;Id&#x27;</span>], test_data[<span class="string">&#x27;SalePrice&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">    submission.to_csv(<span class="string">&#x27;submission.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">train_and_pred(train_features, test_features, train_labels, test_data,</span><br><span class="line">               num_epochs, lr, weight_decay, batch_size)</span><br></pre></td></tr></table></figure>
<p>最后得到的分数值是<strong>0.16696</strong>。</p>
<h2 id="优化改进"><a class="markdownIt-Anchor" href="#优化改进"></a> 优化改进</h2>
<h3 id="引入xgboost"><a class="markdownIt-Anchor" href="#引入xgboost"></a> 引入XGBoost</h3>
<p>在经过Gemini和资料查阅后，我了解到工业界处理表格数据的王者其实是 <strong>Gradient Boosting（梯度提升）</strong>。其中，最常被人使用的是XGBoost模型。</p>
<p>在这里我不打算对决策树和XGBoost的底层原理做详细的展开，如果将来有时间的话，也许我会把它整理成一篇博客。尽管如此，在这里我还是想阐述一些我对它们的浅层理解（参考了知乎文章和吴恩达的《machine-learning》课程）</p>
<p>首先让我们来看看决策树长啥样。如果我们拥有一堆猫和狗的样本，需要根据不同的特征对数据集进行划分，希望得到一个模型来识别输入样本是猫还是狗（一个典型的分类问题），那么我们可以得到下面这棵决策树。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20260115154435207.png?x-oss-process=style/blog" alt="image-20260115154435207" style="zoom:67%;" />
<p>简单地说，决策树模型就是通过不断地回答问题，输出Yes or No，一直向下走直到抵达叶子节点，叶子节点的值就是预测值。</p>
<p>在我们训练一棵决策树时，我们需要选择一个特征作为分裂点，使得<strong>信息增益</strong>最大化。通常可以用递归的方法生成一棵决策树，直到分裂出来的子集合为“<strong>纯净的</strong>”（即 全猫或者全狗）或者到达了树所允许分裂的最大深度就停止分裂。这个信息增益这里不多做探究，本质上就是一个度量节点纯净度的方法，涉及一些很基本的信息论定义。</p>
<p>一棵决策树往往不足以用来解决问题，因为它高度依赖数据集本身来决定用哪个特征分裂以实现信息增益最大化。所以，<strong>袋装决策树</strong>和<strong>随机森林算法</strong>出现了。袋装决策树的核心理念就是：在原始数据集上进行有放回随机抽样得到多个训练数据集，以此训练多棵决策树来进行预测，最后对所有决策树的输出进行投票，来决定最终的预测结果。它的算法伪代码描述如下：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20260115155728813.png?x-oss-process=style/blog" alt="image-20260115155728813" style="zoom:67%;" />
<p>这么做有一个小问题：当<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>非常大时，可能会生成很多个根节点使用相同的分割、或者根节点附近使用相同分割的决策树。<strong>随机森林算法</strong>则是在袋装决策树上进一步优化：随机森林在分裂节点时，<strong>并不是在所有特征中寻找最优解，而是随机抽取一部分特征（通常取 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mi>M</mi></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{M}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.11333499999999996em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9266650000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span><span style="top:-2.886665em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.11333499999999996em;"><span></span></span></span></span></span></span></span></span>，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> 为特征总数）进行选择</strong>。这种特征层面的随机性进一步降低了树与树之间的相关性，让投票结果更健壮。</p>
<p>这么做的好处就在于，输入数据集的任意变化，都不太可能对随机森林模型的预测结果产生巨大的影响，因为它已经对训练数据集的微小变化进行了平均化处理。</p>
<p>最后就是我们的XGBoost模型，XGBoost 属于 <strong>Boosting（提升）</strong> 家族，这与随机森林的 Bagging 思想完全不同。随机森林是<strong>并行</strong>地训练多棵树然后投票，而 XGBoost 是<strong>串行</strong>地训练。它的核心思想是：<strong>每一棵新树的建立，都是为了修正前一棵树的错误。</strong></p>
<p>简单来说，如果第一棵树预测的结果和真实值有差距（这个差距称为残差），那么第二棵树的目标就不再是预测原始数据，而是去拟合这个残差。</p>
<p>这种算法背后的思想直观上也很好理解：就像你做模拟卷，第一次考完后发现导数题丢分了（产生了残差），那么你接下来的复习计划（下一棵树）就<strong>专门针对导数这部分偏差进行修正</strong>，而不是从头再把整张卷子做一遍。这种策略让 XGBoost 能够不断逼近正确结果，也让它成为了 Kaggle 比赛中的夺冠常客。</p>
<p>至于 XGBoost 的底层数学原理这里就不展开赘述。我们只要知道它不同于传统的决策树模型用信息增益最大化作为分裂节点的策略，而是通过泰勒展开用到二阶导数信息，来极小化目标损失函数。这使得它比只运用一阶导数的传统 GBDT 更加精准和高效。详细可以参考文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/562983875">超详细解析XGBoost（你想要的都有） - 知乎</a></p>
<p>对于前置知识的介绍到此为止，接下来就是在模型中引入XGBoost。要修改代码也不难，XGBoost有一个非常方便的开源库可以调用，通过pip下载后，在代码中引入库文件。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br></pre></td></tr></table></figure>
<p>之后，把训练部分替换为以下代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 定义模型 (参数是随手填的，不用细调也能赢 MLP)</span></span><br><span class="line">xgb_model = xgb.XGBRegressor(</span><br><span class="line">    objective=<span class="string">&#x27;reg:squarederror&#x27;</span>,</span><br><span class="line">    n_estimators=<span class="number">1000</span>,     <span class="comment"># 树的数量</span></span><br><span class="line">    learning_rate=<span class="number">0.05</span>,    <span class="comment"># 学习率</span></span><br><span class="line">    max_depth=<span class="number">5</span>,           <span class="comment"># 树的深度</span></span><br><span class="line">    n_jobs=-<span class="number">1</span>              <span class="comment">#以此来动用你所有的CPU核心</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 训练 (记得把 PyTorch 张量转回 numpy)</span></span><br><span class="line"><span class="comment"># train_features 和 train_labels 是你之前处理好的</span></span><br><span class="line"><span class="comment"># reshape(-1) 是为了把标签变成一维数组，XGBoost 喜欢一维的 y</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;开始训练 XGBoost...&quot;</span>)</span><br><span class="line">xgb_model.fit(train_features.numpy(), train_labels.numpy().reshape(-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 预测</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;正在预测...&quot;</span>)</span><br><span class="line">predictions = xgb_model.predict(test_features.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 保存结果</span></span><br><span class="line">test_data[<span class="string">&#x27;SalePrice&#x27;</span>] = pd.Series(predictions)</span><br><span class="line">submission = pd.concat([test_data[<span class="string">&#x27;Id&#x27;</span>], test_data[<span class="string">&#x27;SalePrice&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">submission.to_csv(<span class="string">&#x27;submission_xgb.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>将得到的CSV文件上传到Kaggle，这次的分数值是<strong>0.13312</strong>。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20260115163623779.png?x-oss-process=style/blog" alt="image-20260115163623779" style="zoom:50%;" />
<h3 id="特征工程模型融合"><a class="markdownIt-Anchor" href="#特征工程模型融合"></a> 特征工程＋模型融合</h3>
<p>后续的优化过程就比较单调了，因为我直接让Gemini直接给我优化建议，我很好奇这道题能优化到什么程度。</p>
<p>Gemini给了我两个建议：</p>
<ol>
<li>
<p>引入特征工程，对原训练数据集的特征进行处理，尝试根据直觉构造一些新的强力特征取辅助机器学习。</p>
<blockquote>
<p>比如总面积：地下室 + 一楼 + 二楼</p>
<p>房龄 = 卖出年份 - 建成年份 等</p>
</blockquote>
</li>
<li>
<p>引入模型融合，不要只信一个模型，可以引入多个模型对预测结果取加权平均，来利用各个模型的优点。</p>
<blockquote>
<p>这里Gemini给出的范例是：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Final Price</mtext><mo>=</mo><mn>0.6</mn><mo>×</mo><mtext>XGBoost</mtext><mo>+</mo><mn>0.2</mn><mo>×</mo><mtext>Lasso</mtext><mo>+</mo><mn>0.2</mn><mo>×</mo><mtext>Ridge</mtext></mrow><annotation encoding="application/x-tex">\text{Final Price} = 0.6 \times \text{XGBoost} + 0.2 \times \text{Lasso} + 0.2 \times \text{Ridge}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord text"><span class="mord">Final Price</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord text"><span class="mord">XGBoost</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord text"><span class="mord">Lasso</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">Ridge</span></span></span></span></span></span></p>
</blockquote>
</li>
</ol>
<ul>
<li><strong>XGBoost (树模型)</strong>：绝对的主力（权重 60%）。它擅长捕捉非线性的复杂关系和特征交互，但它的预测本质上是阶梯状的，容易过拟合。</li>
<li><strong>Lasso Regression (L1 正则)</strong>：激进的线性模型。它能将不重要的特征系数压缩为 0（自动做特征选择），负责剔除噪音，防止 XGBoost 在无关特征上钻牛角尖。</li>
<li><strong>Ridge Regression (L2 正则)</strong>：稳健的线性模型。它处理共线性特征（比如多个代表面积的指标），让模型更平滑。</li>
</ul>
<p>之后，修改模型代码（完整代码在文末一起放出），重新训练并生成一个CSV文件提交给Kaggle。这次的得分是<strong>0.12686</strong>。</p>
<h3 id="引入lightgbm"><a class="markdownIt-Anchor" href="#引入lightgbm"></a> 引入LightGBM</h3>
<p>LightGBM 是微软开发的，它和 XGBoost 的切分逻辑不同，两者融合通常能产生奇效。</p>
<p>简单来说，LightGBM就是肉的一批的同时伤害还贼高。</p>
<ol>
<li>模型精度：XGBoost和LightGBM相当。</li>
<li>训练速度：LightGBM远快于XGBoost。(快百倍以上，跟数据集有关系)</li>
<li>内存消耗：LightGBM远小于XGBoost。(大约是XGB的五分之一)</li>
<li>缺失值特征：XGBoost和LightGBM都可以自动处理特征缺失值。</li>
<li>分类特征：XGBoost不支持类别特征，需要OneHot编码预处理。LightGBM直接支持类别特征。</li>
</ol>
<p>最后，我们引入LightGBM并且调整模型权重为：XGB (30%) + LGBM (30%) + Lasso (20%) + Ridge (20%)</p>
<p>训练分数定格在<strong>0.12462</strong>。这还是在未经过任何调参，纯用Gemini给的超参数提交的结果。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20260115173501721.png?x-oss-process=style/blog" alt="image-20260115173501721" /></p>
<h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2>
<p>事实证明，决策树模型比神经网络更加适合处理表格类型的数据。别的不说，哪怕不做特征工程和模型融合，光是引入XGBoost模型，得分就能从0.16优化为0.13，可见XGBoost之威力。</p>
<p>这里插播一段小插曲，在查阅相关资料时，我无意间了解到了机器学习两大流派之间的争论。</p>
<blockquote>
<p>机器学习主要可以分为联结主义（Connectionism）和符号主义（Symbolism，或称统计学习学派）两大流派。前者以 MLP、深度学习为代表，后者则以决策树、XGBoost 等算法为代表。</p>
<p>所谓联结主义，可以理解为通过数据预处理，把研究对象看作一个高维矩阵。它本质上是在寻找一个复杂的函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">f(x) = y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span>，通过反向传播算法不断微调权重矩阵，以最小化目标函数。</p>
<p>它的思维方式是连续的：就像捏泥人，通过基于微积分的梯度下降，一点点把模型捏成想要的形状。但这也带来了弊端——它必须从零开始学习所有规律。哪怕是万有引力这样显而易见的物理定律，在神经网络眼里也只是如果不通过海量样本训练就无法察觉的隐性特征。此外，它的黑盒特性导致人们很难控制其学习过程，容易出现过拟合。</p>
<p>而以树模型为代表的符号主义，它的逻辑截然不同。它不进行复杂的矩阵乘法，而是遵循 If-Else 的硬逻辑。它通过回答一个个离散的问题，把样本空间一步步切割得更纯净。</p>
<p>“在哪里切这一刀”不是人定的，而是机器依靠算法自动找出来的。如果说联结主义是微调，那么树模型就是贪心——无论是随机森林利用信息增益并行生长，还是 XGBoost 利用残差和梯度串行修补，它们都主张把问题离散化，在每一步寻找当下最好的切分点。</p>
<p>从当下的技术热点来看，联结主义无疑占据了统治地位。以 GPT 为代表的大语言模型证明了大力是真的能出奇迹的。通过海量参数和反向传播，机器涌现出了惊人的智能。很多人认为，只要算力足够大，神经网络就能解决一切问题。但与此同时，也有人在反思当下是否走了弯路。毕竟神经网络能给机器带去直觉，却不一定能理解规律，他们认为，符号主义才是通往AGI的正确道路。</p>
</blockquote>
<p>还有一点，在模型选择遇到性能瓶颈后，可以对数据集进行特征工程处理，手动构造出有用的特征值。此外，通过模型融合来综合考虑各个模型的输出结果也能提升表现。这些方法都在这次实战中得到了证实。</p>
<p>如果有时间，最好在本地构造验证集，并尝试不同的超参数，也许能够获得更好的效果。</p>
<p>另外，完整的提交代码如下。一共有三份，分别对应MLP模型，XGBoost模型与模型融合代码。</p>
<details class="folding-tag" cyan><summary> 查看完整代码 </summary>
              <div class='content'>
              <div class="tabs" id="kaggle_house_prices_prediction"><ul class="nav-tabs"><li class="tab"><button type="button" data-href="#kaggle_house_prices_prediction-1">MLP.py</button></li><li class="tab"><button type="button" data-href="#kaggle_house_prices_prediction-2">XGBoost.py</button></li><li class="tab active"><button type="button" data-href="#kaggle_house_prices_prediction-3">Final.py</button></li></ul><div class="tab-contents"><div class="tab-item-content" id="kaggle_house_prices_prediction-1"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tarfile</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line">DATA_HUB = <span class="built_in">dict</span>()</span><br><span class="line">DATA_URL = <span class="string">&#x27;http://d2l-data.s3-accelerate.amazonaws.com/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download</span>(<span class="params">name, cache_dir=os.path.join(<span class="params"><span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span></span>)</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载一个DATA_HUB中的文件，返回本地文件名&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> name <span class="keyword">in</span> DATA_HUB, <span class="string">f&quot;<span class="subst">&#123;name&#125;</span> 不存在于 <span class="subst">&#123;DATA_HUB&#125;</span>&quot;</span></span><br><span class="line">    url, sha1_hash = DATA_HUB[name]</span><br><span class="line">    os.makedirs(cache_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    fname = os.path.join(cache_dir, url.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> os.path.exists(fname):</span><br><span class="line">        sha1 = hashlib.sha1()</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                data = f.read(<span class="number">1048576</span>)</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> data:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                sha1.update(data)</span><br><span class="line">        <span class="keyword">if</span> sha1.hexdigest() == sha1_hash:</span><br><span class="line">            <span class="keyword">return</span> fname  <span class="comment"># 命中缓存</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;正在从<span class="subst">&#123;url&#125;</span>下载<span class="subst">&#123;fname&#125;</span>...&#x27;</span>)</span><br><span class="line">    r = requests.get(url, stream=<span class="literal">True</span>, verify=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(r.content)</span><br><span class="line">    <span class="keyword">return</span> fname</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_extract</span>(<span class="params">name, folder=<span class="literal">None</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载并解压zip/tar文件&quot;&quot;&quot;</span></span><br><span class="line">    fname = download(name)</span><br><span class="line">    base_dir = os.path.dirname(fname)</span><br><span class="line">    data_dir, ext = os.path.splitext(fname)</span><br><span class="line">    <span class="keyword">if</span> ext == <span class="string">&#x27;.zip&#x27;</span>:</span><br><span class="line">        fp = zipfile.ZipFile(fname, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> ext <span class="keyword">in</span> (<span class="string">&#x27;.tar&#x27;</span>, <span class="string">&#x27;.gz&#x27;</span>):</span><br><span class="line">        fp = tarfile.<span class="built_in">open</span>(fname, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="literal">False</span>, <span class="string">&#x27;只有zip/tar文件可以被解压缩&#x27;</span></span><br><span class="line">    fp.extractall(base_dir)</span><br><span class="line">    <span class="keyword">return</span> os.path.join(base_dir, folder) <span class="keyword">if</span> folder <span class="keyword">else</span> data_dir</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_all</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载DATA_HUB中的所有文件&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> DATA_HUB:</span><br><span class="line">        download(name)</span><br><span class="line"></span><br><span class="line">DATA_HUB[<span class="string">&#x27;kaggle_house_train&#x27;</span>] = (  <span class="comment">#@save</span></span><br><span class="line">    DATA_URL + <span class="string">&#x27;kaggle_house_pred_train.csv&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;585e9cc93e70b39160e7921475f9bcd7d31219ce&#x27;</span>)</span><br><span class="line"></span><br><span class="line">DATA_HUB[<span class="string">&#x27;kaggle_house_test&#x27;</span>] = (  <span class="comment">#@save</span></span><br><span class="line">    DATA_URL + <span class="string">&#x27;kaggle_house_pred_test.csv&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;fa19780a7b011d9b009e8bff8e99922a8ee2eb90&#x27;</span>)</span><br><span class="line"></span><br><span class="line">train_data = pd.read_csv(download(<span class="string">&#x27;kaggle_house_train&#x27;</span>))</span><br><span class="line">test_data = pd.read_csv(download(<span class="string">&#x27;kaggle_house_test&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去除ID列表</span></span><br><span class="line">all_features = pd.concat((train_data.iloc[:, <span class="number">1</span>:-<span class="number">1</span>], test_data.iloc[:, <span class="number">1</span>:]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])</span></span><br><span class="line"><span class="comment"># print(all_features.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line"><span class="comment"># 首先将缺失的值替换为相应特征的平均值，然后将特征进行标准化，重新缩放到零均值和单位方差</span></span><br><span class="line">numeric_features = all_features.dtypes[all_features.dtypes != <span class="string">&#x27;object&#x27;</span>].index</span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].apply(</span><br><span class="line">    <span class="keyword">lambda</span> x: (x - x.mean()) / (x.std())</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 标准化数据后，均值为0，故缺失值设置为0</span></span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].fillna(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来，用独热编码替换离散值</span></span><br><span class="line">all_features = pd.get_dummies(all_features, dummy_na=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># all_features.shape</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从pandas格式提取NumPy格式，并转换为张量用于训练</span></span><br><span class="line">n_train = train_data.shape[<span class="number">0</span>]</span><br><span class="line">train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)</span><br><span class="line">test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)</span><br><span class="line">train_labels = torch.tensor(</span><br><span class="line">    train_data.SalePrice.values.reshape(-<span class="number">1</span>, <span class="number">1</span>),dtype=torch.float32</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练一个线性模型，作为baseline</span></span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line">in_features = train_features.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_net</span>():</span><br><span class="line">    net = nn.Sequential(nn.Linear(in_features,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">log_rmse</span>(<span class="params">net, features, labels</span>):</span><br><span class="line">    <span class="comment"># 为了在取对数时进一步稳定该值，将小于1的值设置为1</span></span><br><span class="line">    clipped_preds = torch.clamp(net(features), <span class="number">1</span>, <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br><span class="line">    rmse = torch.sqrt(loss(torch.log(clipped_preds),</span><br><span class="line">                           torch.log(labels)))</span><br><span class="line">    <span class="keyword">return</span> rmse.item()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_features, train_labels, test_features, test_labels,</span></span><br><span class="line"><span class="params">          num_epochs, learning_rate, weight_decay, batch_size</span>):</span><br><span class="line">    train_ls, test_ls =[], []</span><br><span class="line">    train_iter = d2l.load_array((train_features, train_labels), batch_size)</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(),</span><br><span class="line">                                 lr=learning_rate,</span><br><span class="line">                                 weight_decay=weight_decay)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l = loss(net(X),y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">        train_ls.append(log_rmse(net, train_features, train_labels))</span><br><span class="line">        <span class="keyword">if</span> test_labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            test_ls.append(log_rmse(net, test_features, test_labels))</span><br><span class="line">    <span class="keyword">return</span> train_ls, test_ls</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_k_fold_data</span>(<span class="params">k, i, X, y</span>):</span><br><span class="line">    <span class="keyword">assert</span> k &gt; <span class="number">1</span></span><br><span class="line">    fold_size = X.shape[<span class="number">0</span>] // k</span><br><span class="line">    X_train, y_train = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        idx = <span class="built_in">slice</span>(j * fold_size, (j+<span class="number">1</span>) * fold_size)</span><br><span class="line">        X_part, y_part = X[idx, :], y[idx]</span><br><span class="line">        <span class="keyword">if</span> j == i:</span><br><span class="line">            X_valid, y_valid = X_part, y_part</span><br><span class="line">        <span class="keyword">elif</span> X_train <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            X_train, y_train = X_part, y_part</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X_train = torch.cat([X_train, X_part], <span class="number">0</span>)</span><br><span class="line">            y_train = torch.cat([y_train, y_part], <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> X_train, y_train, X_valid, y_valid</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">k_fold</span>(<span class="params">k, X_train, y_train, num_epochs, learning_rate, weight_decay,</span></span><br><span class="line"><span class="params">           batch_size</span>):</span><br><span class="line">    train_l_sum, valid_l_sum = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        data = get_k_fold_data(k, i, X_train, y_train)</span><br><span class="line">        net = get_net()</span><br><span class="line">        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,</span><br><span class="line">                                   weight_decay, batch_size)</span><br><span class="line">        train_l_sum += train_ls[-<span class="number">1</span>]</span><br><span class="line">        valid_l_sum += valid_ls[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            d2l.plot(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>)), [train_ls, valid_ls],</span><br><span class="line">                     xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;rmse&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                     legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>], yscale=<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">            <span class="comment"># 【新增代码】将图像保存到当前目录下的 result.png 文件中</span></span><br><span class="line">            <span class="comment"># plt.savefig(&#x27;result1.png&#x27;)</span></span><br><span class="line">            <span class="comment"># print(&quot;图像已保存为 result1.png&quot;) </span></span><br><span class="line">            </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;折<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>，训练log rmse<span class="subst">&#123;<span class="built_in">float</span>(train_ls[-<span class="number">1</span>]):f&#125;</span>, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;验证log rmse<span class="subst">&#123;<span class="built_in">float</span>(valid_ls[-<span class="number">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class="line">              </span><br><span class="line">    <span class="comment"># 注意：下面这个 return 缩进有问题（见下文“额外提示”）</span></span><br><span class="line">    <span class="keyword">return</span> train_l_sum / k, valid_l_sum /k</span><br><span class="line">    </span><br><span class="line">k, num_epochs, lr, weight_decay, batch_size = <span class="number">5</span>, <span class="number">100</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">64</span></span><br><span class="line">train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,</span><br><span class="line">                          weight_decay, batch_size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;k&#125;</span>-折验证: 平均训练log rmse: <span class="subst">&#123;<span class="built_in">float</span>(train_l):f&#125;</span>, &#x27;</span></span><br><span class="line">      <span class="string">f&#x27;平均验证log rmse: <span class="subst">&#123;<span class="built_in">float</span>(valid_l):f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_and_pred</span>(<span class="params">train_features, test_features, train_labels, test_data,</span></span><br><span class="line"><span class="params">                   num_epochs, lr, weight_decay, batch_size</span>):</span><br><span class="line">    net = get_net()</span><br><span class="line">    train_ls, _ = train(net, train_features, train_labels, <span class="literal">None</span>, <span class="literal">None</span>,</span><br><span class="line">                        num_epochs, lr, weight_decay, batch_size)</span><br><span class="line">    d2l.plot(np.arange(<span class="number">1</span>, num_epochs + <span class="number">1</span>), [train_ls], xlabel=<span class="string">&#x27;epoch&#x27;</span>,</span><br><span class="line">             ylabel=<span class="string">&#x27;log rmse&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], yscale=<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&#x27;train_final.png&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;训练log rmse：<span class="subst">&#123;<span class="built_in">float</span>(train_ls[-<span class="number">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 将网络应用于测试集。</span></span><br><span class="line">    preds = net(test_features).detach().numpy()</span><br><span class="line">    <span class="comment"># 将其重新格式化以导出到Kaggle</span></span><br><span class="line">    test_data[<span class="string">&#x27;SalePrice&#x27;</span>] = pd.Series(preds.reshape(<span class="number">1</span>, -<span class="number">1</span>)[<span class="number">0</span>])</span><br><span class="line">    submission = pd.concat([test_data[<span class="string">&#x27;Id&#x27;</span>], test_data[<span class="string">&#x27;SalePrice&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">    submission.to_csv(<span class="string">&#x27;submission.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">train_and_pred(train_features, test_features, train_labels, test_data,</span><br><span class="line">               num_epochs, lr, weight_decay, batch_size)</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="kaggle_house_prices_prediction-2"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tarfile</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line">DATA_HUB = <span class="built_in">dict</span>()</span><br><span class="line">DATA_URL = <span class="string">&#x27;http://d2l-data.s3-accelerate.amazonaws.com/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download</span>(<span class="params">name, cache_dir=os.path.join(<span class="params"><span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span></span>)</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载一个DATA_HUB中的文件，返回本地文件名&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> name <span class="keyword">in</span> DATA_HUB, <span class="string">f&quot;<span class="subst">&#123;name&#125;</span> 不存在于 <span class="subst">&#123;DATA_HUB&#125;</span>&quot;</span></span><br><span class="line">    url, sha1_hash = DATA_HUB[name]</span><br><span class="line">    os.makedirs(cache_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    fname = os.path.join(cache_dir, url.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> os.path.exists(fname):</span><br><span class="line">        sha1 = hashlib.sha1()</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                data = f.read(<span class="number">1048576</span>)</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> data:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                sha1.update(data)</span><br><span class="line">        <span class="keyword">if</span> sha1.hexdigest() == sha1_hash:</span><br><span class="line">            <span class="keyword">return</span> fname  <span class="comment"># 命中缓存</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;正在从<span class="subst">&#123;url&#125;</span>下载<span class="subst">&#123;fname&#125;</span>...&#x27;</span>)</span><br><span class="line">    r = requests.get(url, stream=<span class="literal">True</span>, verify=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(r.content)</span><br><span class="line">    <span class="keyword">return</span> fname</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_extract</span>(<span class="params">name, folder=<span class="literal">None</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载并解压zip/tar文件&quot;&quot;&quot;</span></span><br><span class="line">    fname = download(name)</span><br><span class="line">    base_dir = os.path.dirname(fname)</span><br><span class="line">    data_dir, ext = os.path.splitext(fname)</span><br><span class="line">    <span class="keyword">if</span> ext == <span class="string">&#x27;.zip&#x27;</span>:</span><br><span class="line">        fp = zipfile.ZipFile(fname, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> ext <span class="keyword">in</span> (<span class="string">&#x27;.tar&#x27;</span>, <span class="string">&#x27;.gz&#x27;</span>):</span><br><span class="line">        fp = tarfile.<span class="built_in">open</span>(fname, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="literal">False</span>, <span class="string">&#x27;只有zip/tar文件可以被解压缩&#x27;</span></span><br><span class="line">    fp.extractall(base_dir)</span><br><span class="line">    <span class="keyword">return</span> os.path.join(base_dir, folder) <span class="keyword">if</span> folder <span class="keyword">else</span> data_dir</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_all</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载DATA_HUB中的所有文件&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> DATA_HUB:</span><br><span class="line">        download(name)</span><br><span class="line"></span><br><span class="line">DATA_HUB[<span class="string">&#x27;kaggle_house_train&#x27;</span>] = (  <span class="comment">#@save</span></span><br><span class="line">    DATA_URL + <span class="string">&#x27;kaggle_house_pred_train.csv&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;585e9cc93e70b39160e7921475f9bcd7d31219ce&#x27;</span>)</span><br><span class="line"></span><br><span class="line">DATA_HUB[<span class="string">&#x27;kaggle_house_test&#x27;</span>] = (  <span class="comment">#@save</span></span><br><span class="line">    DATA_URL + <span class="string">&#x27;kaggle_house_pred_test.csv&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;fa19780a7b011d9b009e8bff8e99922a8ee2eb90&#x27;</span>)</span><br><span class="line"></span><br><span class="line">train_data = pd.read_csv(download(<span class="string">&#x27;kaggle_house_train&#x27;</span>))</span><br><span class="line">test_data = pd.read_csv(download(<span class="string">&#x27;kaggle_house_test&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去除ID列表</span></span><br><span class="line">all_features = pd.concat((train_data.iloc[:, <span class="number">1</span>:-<span class="number">1</span>], test_data.iloc[:, <span class="number">1</span>:]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line"><span class="comment"># 首先将缺失的值替换为相应特征的平均值，然后将特征进行标准化，重新缩放到零均值和单位方差</span></span><br><span class="line">numeric_features = all_features.dtypes[all_features.dtypes != <span class="string">&#x27;object&#x27;</span>].index</span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].apply(</span><br><span class="line">    <span class="keyword">lambda</span> x: (x - x.mean()) / (x.std())</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 标准化数据后，均值为0，故缺失值设置为0</span></span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].fillna(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来，用独热编码替换离散值</span></span><br><span class="line">all_features = pd.get_dummies(all_features, dummy_na=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># all_features.shape</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从pandas格式提取NumPy格式，并转换为张量用于训练</span></span><br><span class="line">n_train = train_data.shape[<span class="number">0</span>]</span><br><span class="line">train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)</span><br><span class="line">test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)</span><br><span class="line">train_labels = torch.tensor(</span><br><span class="line">    train_data.SalePrice.values.reshape(-<span class="number">1</span>, <span class="number">1</span>),dtype=torch.float32</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 定义模型 (参数是随手填的，不用细调也能赢 MLP)</span></span><br><span class="line">xgb_model = xgb.XGBRegressor(</span><br><span class="line">    objective=<span class="string">&#x27;reg:squarederror&#x27;</span>,</span><br><span class="line">    n_estimators=<span class="number">1000</span>,     <span class="comment"># 树的数量</span></span><br><span class="line">    learning_rate=<span class="number">0.05</span>,    <span class="comment"># 学习率</span></span><br><span class="line">    max_depth=<span class="number">5</span>,           <span class="comment"># 树的深度</span></span><br><span class="line">    n_jobs=-<span class="number">1</span>              <span class="comment">#以此来动用你所有的CPU核心</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 训练 (记得把 PyTorch 张量转回 numpy)</span></span><br><span class="line"><span class="comment"># train_features 和 train_labels 是你之前处理好的</span></span><br><span class="line"><span class="comment"># reshape(-1) 是为了把标签变成一维数组，XGBoost 喜欢一维的 y</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;开始训练 XGBoost...&quot;</span>)</span><br><span class="line">xgb_model.fit(train_features.numpy(), train_labels.numpy().reshape(-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 预测</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;正在预测...&quot;</span>)</span><br><span class="line">predictions = xgb_model.predict(test_features.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 保存结果</span></span><br><span class="line">test_data[<span class="string">&#x27;SalePrice&#x27;</span>] = pd.Series(predictions)</span><br><span class="line">submission = pd.concat([test_data[<span class="string">&#x27;Id&#x27;</span>], test_data[<span class="string">&#x27;SalePrice&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">submission.to_csv(<span class="string">&#x27;submission_xgb.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;搞定！去提交 submission_xgb.csv 吧！&quot;</span>)</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content active" id="kaggle_house_prices_prediction-3"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, cross_val_score</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb  <span class="comment"># 引入新巨头</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 忽略一些恼人的警告</span></span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 1. 读取数据 (假设数据在 ../data/ 目录下，根据实际情况修改)</span></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;正在读取数据...&quot;</span>)</span><br><span class="line">train_path = <span class="string">&#x27;../data/kaggle_house_pred_train.csv&#x27;</span></span><br><span class="line">test_path = <span class="string">&#x27;../data/kaggle_house_pred_test.csv&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果找不到文件，尝试在当前目录找</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    train_data = pd.read_csv(train_path)</span><br><span class="line">    test_data = pd.read_csv(test_path)</span><br><span class="line"><span class="keyword">except</span> FileNotFoundError:</span><br><span class="line">    train_data = pd.read_csv(<span class="string">&#x27;kaggle_house_pred_train.csv&#x27;</span>)</span><br><span class="line">    test_data = pd.read_csv(<span class="string">&#x27;kaggle_house_pred_test.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 2. 特征工程 (Feature Engineering) - 这里的每一行都是分数的来源</span></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;正在进行高级特征工程...&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去掉训练集中的极端离群点 (Outliers)，这是数据科学界的共识</span></span><br><span class="line"><span class="comment"># 比如有些房子面积特别大(&gt;4000)但价格却很便宜，这种数据会误导模型</span></span><br><span class="line">train_data = train_data[train_data.GrLivArea &lt; <span class="number">4500</span>]</span><br><span class="line">train_data.reset_index(drop=<span class="literal">True</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 记录训练集数量，准备拼接</span></span><br><span class="line">n_train = train_data.shape[<span class="number">0</span>]</span><br><span class="line">train_y = np.log1p(train_data.SalePrice.values) <span class="comment"># 标签 Log 变换</span></span><br><span class="line">all_data = pd.concat((train_data.iloc[:, <span class="number">1</span>:-<span class="number">1</span>], test_data.iloc[:, <span class="number">1</span>:]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 构造强力新特征 ---</span></span><br><span class="line"><span class="comment"># 1. 总面积：地下室 + 一楼 + 二楼。这是决定房价最核心的因素</span></span><br><span class="line">all_data[<span class="string">&#x27;TotalSF&#x27;</span>] = all_data[<span class="string">&#x27;TotalBsmtSF&#x27;</span>] + all_data[<span class="string">&#x27;1stFlrSF&#x27;</span>] + all_data[<span class="string">&#x27;2ndFlrSF&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 浴室总数：全浴 + 半浴*0.5</span></span><br><span class="line">all_data[<span class="string">&#x27;Total_Bathrooms&#x27;</span>] = (all_data[<span class="string">&#x27;FullBath&#x27;</span>] + (<span class="number">0.5</span> * all_data[<span class="string">&#x27;HalfBath&#x27;</span>]) +</span><br><span class="line">                               all_data[<span class="string">&#x27;BsmtFullBath&#x27;</span>] + (<span class="number">0.5</span> * all_data[<span class="string">&#x27;BsmtHalfBath&#x27;</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 房子综合素质：总体评价 * 总面积 (交互特征)</span></span><br><span class="line">all_data[<span class="string">&#x27;Total_SF_Qual&#x27;</span>] = all_data[<span class="string">&#x27;TotalSF&#x27;</span>] * all_data[<span class="string">&#x27;OverallQual&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 有无泳池/地下室/二楼 (布尔特征)</span></span><br><span class="line">all_data[<span class="string">&#x27;HasPool&#x27;</span>] = all_data[<span class="string">&#x27;PoolArea&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line">all_data[<span class="string">&#x27;Has2ndFloor&#x27;</span>] = all_data[<span class="string">&#x27;2ndFlrSF&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line">all_data[<span class="string">&#x27;HasGarage&#x27;</span>] = all_data[<span class="string">&#x27;GarageArea&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 房龄特征</span></span><br><span class="line">all_data[<span class="string">&#x27;YrBltAndRemod&#x27;</span>] = all_data[<span class="string">&#x27;YearBuilt&#x27;</span>] + all_data[<span class="string">&#x27;YearRemodAdd&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 数据清洗与编码 ---</span></span><br><span class="line"><span class="comment"># 填补缺失值 (不同类型的列用不同策略)</span></span><br><span class="line"><span class="comment"># 文本列缺省通常意味着“没有”，比如 GarageType 缺省就是没有车库</span></span><br><span class="line">cols_fillna_none = [<span class="string">&#x27;PoolQC&#x27;</span>, <span class="string">&#x27;MiscFeature&#x27;</span>, <span class="string">&#x27;Alley&#x27;</span>, <span class="string">&#x27;Fence&#x27;</span>, <span class="string">&#x27;FireplaceQu&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;GarageType&#x27;</span>, <span class="string">&#x27;GarageFinish&#x27;</span>, <span class="string">&#x27;GarageQual&#x27;</span>, <span class="string">&#x27;GarageCond&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;BsmtQual&#x27;</span>, <span class="string">&#x27;BsmtCond&#x27;</span>, <span class="string">&#x27;BsmtExposure&#x27;</span>, <span class="string">&#x27;BsmtFinType1&#x27;</span>, <span class="string">&#x27;BsmtFinType2&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> cols_fillna_none:</span><br><span class="line">    all_data[col] = all_data[col].fillna(<span class="string">&#x27;None&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数值列缺省通常填 0</span></span><br><span class="line">cols_fillna_0 = [<span class="string">&#x27;GarageYrBlt&#x27;</span>, <span class="string">&#x27;GarageArea&#x27;</span>, <span class="string">&#x27;GarageCars&#x27;</span>, <span class="string">&#x27;BsmtFinSF1&#x27;</span>, <span class="string">&#x27;BsmtFinSF2&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;BsmtUnfSF&#x27;</span>,<span class="string">&#x27;TotalBsmtSF&#x27;</span>, <span class="string">&#x27;BsmtFullBath&#x27;</span>, <span class="string">&#x27;BsmtHalfBath&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> cols_fillna_0:</span><br><span class="line">    all_data[col] = all_data[col].fillna(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 其他零散的缺失值填众数 (最常见的值)</span></span><br><span class="line">all_data[<span class="string">&#x27;MSZoning&#x27;</span>] = all_data[<span class="string">&#x27;MSZoning&#x27;</span>].fillna(all_data[<span class="string">&#x27;MSZoning&#x27;</span>].mode()[<span class="number">0</span>])</span><br><span class="line">all_data[<span class="string">&#x27;Electrical&#x27;</span>] = all_data[<span class="string">&#x27;Electrical&#x27;</span>].fillna(all_data[<span class="string">&#x27;Electrical&#x27;</span>].mode()[<span class="number">0</span>])</span><br><span class="line">all_data[<span class="string">&#x27;KitchenQual&#x27;</span>] = all_data[<span class="string">&#x27;KitchenQual&#x27;</span>].fillna(all_data[<span class="string">&#x27;KitchenQual&#x27;</span>].mode()[<span class="number">0</span>])</span><br><span class="line">all_data[<span class="string">&#x27;Exterior1st&#x27;</span>] = all_data[<span class="string">&#x27;Exterior1st&#x27;</span>].fillna(all_data[<span class="string">&#x27;Exterior1st&#x27;</span>].mode()[<span class="number">0</span>])</span><br><span class="line">all_data[<span class="string">&#x27;Exterior2nd&#x27;</span>] = all_data[<span class="string">&#x27;Exterior2nd&#x27;</span>].fillna(all_data[<span class="string">&#x27;Exterior2nd&#x27;</span>].mode()[<span class="number">0</span>])</span><br><span class="line">all_data[<span class="string">&#x27;SaleType&#x27;</span>] = all_data[<span class="string">&#x27;SaleType&#x27;</span>].fillna(all_data[<span class="string">&#x27;SaleType&#x27;</span>].mode()[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 独热编码</span></span><br><span class="line">all_data = pd.get_dummies(all_data).fillna(<span class="number">0</span>) <span class="comment"># 最后的保险，把剩下的 NaN 填 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拆分回训练集和测试集</span></span><br><span class="line">X_train = all_data[:n_train]</span><br><span class="line">X_test = all_data[n_train:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 3. 定义模型融合 (Model Stacking/Blending) - 进阶版</span></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb  <span class="comment"># 引入新巨头</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;准备四大模型...&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型 1: Ridge (线性)</span></span><br><span class="line">ridge = Ridge(alpha=<span class="number">13</span>) <span class="comment"># 稍微调了一下 alpha</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型 2: Lasso (线性)</span></span><br><span class="line">lasso = Lasso(alpha=<span class="number">0.0005</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型 3: XGBoost (树)</span></span><br><span class="line">xgb_model = xgb.XGBRegressor(</span><br><span class="line">    objective=<span class="string">&#x27;reg:squarederror&#x27;</span>,</span><br><span class="line">    n_estimators=<span class="number">3000</span>,</span><br><span class="line">    learning_rate=<span class="number">0.01</span>,</span><br><span class="line">    max_depth=<span class="number">4</span>,</span><br><span class="line">    min_child_weight=<span class="number">1</span>,</span><br><span class="line">    gamma=<span class="number">0</span>,</span><br><span class="line">    subsample=<span class="number">0.7</span>,</span><br><span class="line">    colsample_bytree=<span class="number">0.7</span>,</span><br><span class="line">    n_jobs=-<span class="number">1</span>,</span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型 4: LightGBM (树 - 新加入)</span></span><br><span class="line"><span class="comment"># LightGBM 对叶子节点的生长策略不同，能捕捉 XGB 漏掉的信息</span></span><br><span class="line">lgb_model = lgb.LGBMRegressor(</span><br><span class="line">    objective=<span class="string">&#x27;regression&#x27;</span>,</span><br><span class="line">    num_leaves=<span class="number">31</span>,</span><br><span class="line">    learning_rate=<span class="number">0.01</span>,</span><br><span class="line">    n_estimators=<span class="number">3000</span>,</span><br><span class="line">    max_bin=<span class="number">200</span>,</span><br><span class="line">    bagging_fraction=<span class="number">0.75</span>,</span><br><span class="line">    bagging_freq=<span class="number">5</span>,</span><br><span class="line">    bagging_seed=<span class="number">7</span>,</span><br><span class="line">    feature_fraction=<span class="number">0.2</span>,</span><br><span class="line">    feature_fraction_seed=<span class="number">7</span>,</span><br><span class="line">    verbose=-<span class="number">1</span>,</span><br><span class="line">    n_jobs=-<span class="number">1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 4. 训练与预测</span></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练 Ridge...&quot;</span>)</span><br><span class="line">ridge.fit(X_train, train_y)</span><br><span class="line">ridge_pred = np.expm1(ridge.predict(X_test))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练 Lasso...&quot;</span>)</span><br><span class="line">lasso.fit(X_train, train_y)</span><br><span class="line">lasso_pred = np.expm1(lasso.predict(X_test))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练 XGBoost...&quot;</span>)</span><br><span class="line">xgb_model.fit(X_train, train_y)</span><br><span class="line">xgb_pred = np.expm1(xgb_model.predict(X_test))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练 LightGBM...&quot;</span>)</span><br><span class="line">lgb_model.fit(X_train, train_y)</span><br><span class="line">lgb_pred = np.expm1(lgb_model.predict(X_test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 5. 终极四模型融合 (Blending)</span></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;正在融合四大天王...&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 权重分配策略：树模型负责强攻，线性模型负责修正</span></span><br><span class="line"><span class="comment"># 0.3 * XGB + 0.3 * LGB + 0.2 * Lasso + 0.2 * Ridge</span></span><br><span class="line">final_pred = (<span class="number">0.3</span> * xgb_pred) + (<span class="number">0.3</span> * lgb_pred) + (<span class="number">0.2</span> * lasso_pred) + (<span class="number">0.2</span> * ridge_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 6. 保存结果</span></span><br><span class="line"><span class="comment"># -----------------------------------------------------------</span></span><br><span class="line">submission = pd.DataFrame()</span><br><span class="line">submission[<span class="string">&#x27;Id&#x27;</span>] = test_data.Id</span><br><span class="line">submission[<span class="string">&#x27;SalePrice&#x27;</span>] = final_pred</span><br><span class="line"></span><br><span class="line">filename = <span class="string">&#x27;submission_final_4models.csv&#x27;</span></span><br><span class="line">submission.to_csv(filename, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\n成功生成: <span class="subst">&#123;filename&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;融合策略: XGB(30%) + LGB(30%) + Lasso(20%) + Ridge(20%)&quot;</span>)</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>
              </div>
            </details>
<p>最后，放一下提交记录截图。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20260115173548437.png?x-oss-process=style/blog" alt="image-20260115173548437" /></p>
<hr />
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20260115192208544.png?x-oss-process=style/blog" alt="image-20260115192208544" /></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://adamben.top">Adam Ben</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://adamben.top/posts/2ed6902d017c/">https://adamben.top/posts/2ed6902d017c/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://adamben.top" target="_blank">Adam8enの8log</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Linear-Model/">Linear Model</a><a class="post-meta__tags" href="/tags/MLP/">MLP</a><a class="post-meta__tags" href="/tags/XGBoost/">XGBoost</a></div><div class="post_share"><div class="social-share" data-image="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20260115192208544.png?x-oss-process=style/blog" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/wechat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/posts/8aa53a259f2b/" title="深入理解 Kitsune 模型"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20250511224410700.png?x-oss-process=style/blog" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">深入理解 Kitsune 模型</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Adam Ben</div><div class="author-info__description">你的孤独与月为伴，你的深情与星同归</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">71</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">92</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Adam8en"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Adam8en" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:adamben@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://blog.csdn.net/Adam_Ben?type=blog" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=2813879949&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">不知道说什么好,<br>放个表情包在这里好了,<br>⁽˙³˙⁾◟(๑•́ ₃ •̀๑)◞⁽˙³˙⁾,<br><br>如有疑问欢迎联系邮箱：<br>adamben@qq.com</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#d2l%E7%9A%84%E5%81%9A%E6%B3%95"><span class="toc-number">1.</span> <span class="toc-text"> 《D2L》的做法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.1.</span> <span class="toc-text"> 数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">1.1.1.</span> <span class="toc-text"> 数据标准化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E7%A6%BB%E6%95%A3%E5%80%BC"><span class="toc-number">1.1.2.</span> <span class="toc-text"> 处理离散值</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">1.2.</span> <span class="toc-text"> 模型训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E7%BB%88%E7%BB%93%E6%9E%9C"><span class="toc-number">1.3.</span> <span class="toc-text"> 最终结果</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E6%94%B9%E8%BF%9B"><span class="toc-number">2.</span> <span class="toc-text"> 优化改进</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%95%E5%85%A5xgboost"><span class="toc-number">2.1.</span> <span class="toc-text"> 引入XGBoost</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88"><span class="toc-number">2.2.</span> <span class="toc-text"> 特征工程＋模型融合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%95%E5%85%A5lightgbm"><span class="toc-number">2.3.</span> <span class="toc-text"> 引入LightGBM</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AF%AD"><span class="toc-number">3.</span> <span class="toc-text"> 结语</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/2ed6902d017c/" title="从 Linear Model 到 Stacking，我是如何将 RMSLE 优化至 0.12 的？"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20260115192208544.png?x-oss-process=style/blog" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="从 Linear Model 到 Stacking，我是如何将 RMSLE 优化至 0.12 的？"/></a><div class="content"><a class="title" href="/posts/2ed6902d017c/" title="从 Linear Model 到 Stacking，我是如何将 RMSLE 优化至 0.12 的？">从 Linear Model 到 Stacking，我是如何将 RMSLE 优化至 0.12 的？</a><time datetime="2026-01-15T12:06:12.000Z" title="发表于 2026-01-15 20:06:12">2026-01-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/8aa53a259f2b/" title="深入理解 Kitsune 模型"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20250511224410700.png?x-oss-process=style/blog" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="深入理解 Kitsune 模型"/></a><div class="content"><a class="title" href="/posts/8aa53a259f2b/" title="深入理解 Kitsune 模型">深入理解 Kitsune 模型</a><time datetime="2025-05-11T14:45:55.000Z" title="发表于 2025-05-11 22:45:55">2025-05-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/709e8bfc712b/" title="2024年度总结"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/58183995_p12.jpg?x-oss-process=style/blog" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2024年度总结"/></a><div class="content"><a class="title" href="/posts/709e8bfc712b/" title="2024年度总结">2024年度总结</a><time datetime="2024-12-31T13:29:35.000Z" title="发表于 2024-12-31 21:29:35">2024-12-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/c5de366d06a7/" title="Rolling Hashes：滚动哈希详解"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/2404CBF21EC9ECF50FF185A745A075DE.jpg?x-oss-process=style/blog" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Rolling Hashes：滚动哈希详解"/></a><div class="content"><a class="title" href="/posts/c5de366d06a7/" title="Rolling Hashes：滚动哈希详解">Rolling Hashes：滚动哈希详解</a><time datetime="2024-12-11T12:22:13.000Z" title="发表于 2024-12-11 20:22:13">2024-12-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/15adc5743338/" title="11月总结·2024"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/6b65aa1528d546543f3cd19e50aceb09.jpeg?x-oss-process=style/blog" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="11月总结·2024"/></a><div class="content"><a class="title" href="/posts/15adc5743338/" title="11月总结·2024">11月总结·2024</a><time datetime="2024-11-30T12:40:37.000Z" title="发表于 2024-11-30 20:40:37">2024-11-30</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://adam8en-blog-image.oss-cn-guangzhou.aliyuncs.com/image-20260115192208544.png?x-oss-process=style/blog')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2026 By Adam Ben</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><a class="icon-V hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><svg width="25" height="25" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://npm.elemecdn.com/pangu@4.0.7/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addModeChange('mermaid', runMermaid)

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '07g7RVgbp0tb6j6gBjcZCTKv-gzGzoHsz',
      appKey: '4w4tpzByVtxnDriO8BFDKLBt',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><canvas id="universe"></canvas><script src="/js/universe.js"></script><script src="/js/sun_moon.js" async></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="true" data-text="富强,民主,文明,和谐,平等,公正,法治,爱国,敬业,诚信,友善" data-fontsize="15px" data-random="true" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":200,"height":400},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>